{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfaaf5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\selenium_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "\n",
      "   ------------- -------------------------- 1/3 [prawcore]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   -------------------------- ------------- 2/3 [praw]\n",
      "   ---------------------------------------- 3/3 [praw]\n",
      "\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar la librería PRAW\n",
    "pip install praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0518a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: COLLECT DATA AND STORAGE\n",
    "# Collect posts from SubReddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "570ac0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics: guardados 20 posts en data/posts_politics_hot.csv\n",
      "PoliticalDiscussion: guardados 20 posts en data/posts_PoliticalDiscussion_hot.csv\n",
      "worldnews: guardados 20 posts en data/posts_worldnews_hot.csv\n"
     ]
    }
   ],
   "source": [
    "# Librerías\n",
    "import csv # permite escribir archivos csv\n",
    "from pathlib import Path # servirá para crear una carpeta en la cual guardar los archivos csv\n",
    "import praw\n",
    "\n",
    "# Credenciales\n",
    "CLIENT_ID = \"w2JQaemojvp2ZhiLh7XWig\"\n",
    "CLIENT_SECRET = \"fKXbRxQ4GprlHPudMDnb8HKr-CHxkQ\"\n",
    "USER_AGENT = \"python:webscrapingProject:v1.0 (by /u/Ivan_RZ)\"\n",
    "\n",
    "# Conexión en modo lectura\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT) # Crea objeto 'reddit' para leer datos públicos en Reddit. La librería PRAW se ha descargado para lograr este acceso\n",
    "\n",
    "# Creammos una función de nombre collect_posts. En la primera línea, le dices qué tipo de elemento esperar en subreddit_name, y qué tipo esperar en mode, y así.\n",
    "def collect_posts(subreddit_name: str, mode: str = \"hot\", n: int = 20, time_filter: str = \"day\"): # Con day, le dices a la función que busque los \"top\" de hoy (no aplica para \"hot\")\n",
    "    sub = reddit.subreddit(subreddit_name) # Para buscar subreddits dentro del objeto reddit\n",
    "    gen = sub.hot(limit=None) if mode == \"hot\" else sub.top(time_filter=time_filter, limit=None) # Le pide al subreddit posteos 'hot' y 'top'.\n",
    "    # Nótese que para 'hot' se usa el time_filter de día. El limit es 'None', pero se detendrá en 20 por 'n:int = 20'\n",
    "    rows = [] # Crea la lista para ir metiendo cada post\n",
    "    for s in gen:\n",
    "        if getattr(s, \"stickied\", False): # stickied hace referencia a los posts pinneados (p.e. reglas de un feed). \n",
    "            continue # Si el post es pinneado, lo saltas con 'continue'. Si no está pinneado, la función lo procesa.\n",
    "        rows.append({ # Agrega un diccionario con los cinco elementos que pude el ejercicio: id, title, score...\n",
    "            \"id\": s.id,\n",
    "            \"title\": s.title or \"\", # Las comillas están por si el string no tiene caracteres\n",
    "            \"score\": int(getattr(s, \"score\", 0) or 0), # Usa 0 si el post de reddit no tiene score. 'int' es para que sea integer\n",
    "            \"num_comments\": int(getattr(s, \"num_comments\", 0) or 0), # Misma lógica que el anterior\n",
    "            \"url\": s.url or \"\", # Las comillas están por si aparece vacío el enlace\n",
    "        })\n",
    "        if len(rows) >= n:\n",
    "            break # Termina en 20\n",
    "    return rows\n",
    "\n",
    "# Función save_csv va a recibir los rows y el path que sea creará en la computadora. Para esto último nos sirve la librería csv.\n",
    "def save_csv(rows, path):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True) # Verifica que exista la carpeta del destino, y la crea si es que no existe.\n",
    "    fieldnames = [\"id\",\"title\",\"score\",\"num_comments\",\"url\"] # Define el orden de las columnas en csv.\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f: # Con esto se abre el path para escribir allí archivos csv\n",
    "        w = csv.DictWriter(\n",
    "            f, # 'f' es objeto archivo; su nombre es arbitrario\n",
    "            fieldnames=fieldnames,  # Define el orden de las columnas\n",
    "            delimiter=\";\",             # Excel (en lenguaje ES) espera ;\n",
    "            quoting=csv.QUOTE_MINIMAL  # pone comillas cuando haga falta (si el texto tiene salto de línea o comillas internas)\n",
    "        )\n",
    "        w.writeheader() # Escribe los encabezados\n",
    "        w.writerows(rows) # Escribe todas las filas\n",
    "\n",
    "if __name__ == \"__main__\": # Activa todo el código siguiente, partiendo de que estamos ejecutando directamente desde un notebook.\n",
    "    subreddits = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"] # Nombre de los foros que pide el ejercicio\n",
    "    MODE = \"hot\" # Como la indicación dice seleccionar 20 posteos 'hot' o 'top', se entiendo que puede ser cualquiera de esas. Se está seleccionando 'hot'\n",
    "    TIME_FILTER = \"day\"\n",
    "\n",
    "    for sub in subreddits: # 'sub' tendrá tres valores: politics, PoliticalDiscussion y worldnews\n",
    "        rows = collect_posts(sub, mode=MODE, n=20, time_filter=TIME_FILTER) # Recoge 20 post de cada subreddit\n",
    "        out = ( # Construye el nombre del archivo de salida. Saldrán tres archivos porque sub tiene tres valores.\n",
    "            f\"data/posts_{sub}_{MODE}.csv\"\n",
    "            if MODE != \"top\" else\n",
    "            f\"data/posts_{sub}_{MODE}_{TIME_FILTER}.csv\"\n",
    "        )\n",
    "        save_csv(rows, out)\n",
    "        print(f\"{sub}: guardados {len(rows)} posts en {out}\") # Guarda cada archivo en la computadora y les pone nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f48af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar todo combinado en un solo archivo csv\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "all_rows = [] # Crea una lista para poner allí todos los posts de los subreddits\n",
    "for sub in subreddits:\n",
    "    rows_sub = collect_posts(sub, mode=MODE, n=20, time_filter=TIME_FILTER) # Llama a la función collect_posts\n",
    "    for r in rows_sub: # A cada post ('r') le añadimos la columna 'subreddit', para saber de dónde viene cada post\n",
    "        r[\"subreddit\"] = sub # Define el valor de la columna\n",
    "    all_rows.extend(rows_sub) # Agregar todos los elementos de 'rows_sub' a 'all_rows'\n",
    "\n",
    "Path(\"data\").mkdir(exist_ok=True) # Verifica que exista la carpeta del destino, y la crea si es que no existe.\n",
    "\n",
    "with open(\"data/posts_combined.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    w = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\"subreddit\",\"id\",\"title\",\"score\",\"num_comments\",\"url\"],\n",
    "        delimiter=\";\",    \n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    w.writeheader()\n",
    "    w.writerows(all_rows) # Todo este código escribe el csv combinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eb8ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics: guardados 25 comentarios en data/comments_politics.csv\n",
      "PoliticalDiscussion: guardados 25 comentarios en data/comments_PoliticalDiscussion.csv\n",
      "worldnews: guardados 25 comentarios en data/comments_worldnews.csv\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "# Función que toma los tres archivos (uno por subreddit) y devuelve los cinco con mayor score.\n",
    "def load_top_post_ids_from_csv(csv_path, k=5):\n",
    "    \"\"\"Carga el CSV de posts y devuelve los IDs de los k posts con mayor score.\"\"\"\n",
    "    rows = [] # Crea la lista donde se guardan los csv\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        r = csv.DictReader(f, delimiter=\";\") # Lee cada fila del csv y arroja un diccionario por fila\n",
    "        for row in r: # Recorre cada fila de la posición y convierte el score a integer\n",
    "            try:\n",
    "                row[\"score\"] = int(row.get(\"score\", 0))\n",
    "            except Exception:\n",
    "                row[\"score\"] = 0\n",
    "            rows.append(row) # Agrega la fila con score convertido a la lista 'rows'\n",
    "    rows.sort(key=lambda x: x[\"score\"], reverse=True) # Ordena la lista de mayor a menor en función del score de cada fila\n",
    "    return [r[\"id\"] for r in rows[:k]] # Devuelve los id de las k filas ya ordenadas\n",
    "\n",
    "# Función que pasa los id de la función anterior, establece que se quieren cinco comentarios y el orden por score ('top')\n",
    "def collect_top_comments_for_post(post_id, max_comments=5, sort=\"top\"):\n",
    "    \"\"\"\n",
    "    Trae hasta max_comments comentarios (en cualquier nivel), ordenados por score descendente.\n",
    "    Devuelve dicts: post_id, body, score\n",
    "    \"\"\"\n",
    "    subm = reddit.submission(id=post_id) # Crea el objeto submission ('subm') a partir de su id\n",
    "    subm.comment_sort = sort # Indica el orden de los comments; ya dijimos que será según la categoría 'top'\n",
    "    subm.comments.replace_more(limit=None)  # incluye también los comentarios que están en 'MoreComments' en la web\n",
    "\n",
    "    # Lo siguiente hace que todos los comentarios estén al mismo nivel (como si todos nacieron del mismo post)\n",
    "    flat = []\n",
    "    for c in subm.comments.list(): # Código que efectivamente pone todos los comentarios al mismo nivel\n",
    "        if not hasattr(c, \"body\"):\n",
    "            continue # Los objetos que no tienen .body (del objeto 'comment' en la API) no son comentarios; este código hace que la función se los salte\n",
    "        body = (c.body or \"\").strip() # Si c.body tiene texto devuelve el comentario, sino devuelve \"\". Luego,'strip' recorta espacios o saltos de línea. \n",
    "        if not body or body.lower() in (\"[removed]\", \"[deleted]\"):\n",
    "            continue # Deje de lado aquellos comentarios que se han removido por moderación o se han borrado.\n",
    "        flat.append({ # Guarda el comentarios en la lista 'flat'\n",
    "            \"post_id\": post_id,\n",
    "            \"body\": body.replace(\"\\n\", \" \"),\n",
    "            \"score\": int(getattr(c, \"score\", 0) or 0),\n",
    "        })\n",
    "    flat.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return flat[:max_comments] # Te arroja los cinco comentarios de mayor score del hilo completo\n",
    "\n",
    "# Función para abrir el path y guardar allí los archivos.\n",
    "def save_comments_csv(rows, path):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"post_id\", \"body\", \"score\"],\n",
    "            delimiter=\";\",\n",
    "            quoting=csv.QUOTE_MINIMAL,\n",
    "        )\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\": # Activa todo el código siguiente, partiendo de que estamos ejecutando directamente desde un notebook.\n",
    "    subreddits = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"] # Nombre de los foros que pide el ejercicio\n",
    "    MODE = \"hot\" # Como la indicación dice seleccionar 20 posteos 'hot' o 'top', se entiendo que puede ser cualquiera de esas. Se está seleccionando 'hot'       \n",
    "    TIME_FILTER = \"day\"  # solo aplica si MODE == \"top\"\n",
    "\n",
    "    for sub in subreddits: # 'sub' tendrá tres valores: politics, PoliticalDiscussion y worldnews\n",
    "        posts_csv = (\n",
    "            f\"data/posts_{sub}_{MODE}.csv\"\n",
    "            if MODE != \"top\" else\n",
    "            f\"data/posts_{sub}_{MODE}_{TIME_FILTER}.csv\") # Le pone nombre a los archivos, en función del mode ('top' o 'hot')\n",
    "\n",
    "        # Elegimos el conjunto de posts más relevantes (top 5 por score), del objeto 'posts_csv'\n",
    "        top_post_ids = load_top_post_ids_from_csv(posts_csv, k=5)\n",
    "\n",
    "        # Esto hace que nos quedemos con 5 comentarios por post\n",
    "        all_comments = []\n",
    "        for pid in top_post_ids: # Recorre los id de los cincos mejores posts\n",
    "            comments = collect_top_comments_for_post(pid, max_comments=5, sort=\"top\") # Pide hasta cinco comentarios\n",
    "            all_comments.extend(comments) # Agrega los comentarios\n",
    "\n",
    "        # Guardar comentarios en tres archivos csv (uno por subreddit)\n",
    "        out_csv = f\"data/comments_{sub}.csv\"\n",
    "        save_comments_csv(all_comments, out_csv)\n",
    "        print(f\"{sub}: guardados {len(all_comments)} comentarios en {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b625c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentarios linkeados guardados en: data/comments_linked.csv (filas: 75, huérfanos omitidos: 0)\n"
     ]
    }
   ],
   "source": [
    "# Emparentar comentarios con sus posteos madre\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Función que creará un csv para que cada comentario quede enlazado a su post madre\n",
    "def build_linked_comments_csv(subreddits, mode=\"hot\", time_filter=\"day\",\n",
    "                              out_path=\"data/comments_linked.csv\"):\n",
    "    post_index = {}  # Asocia el id de cada post con un diccionario (subreddit:..., title:...)\n",
    "    for sub in subreddits: # Recorre la lista de subreddits\n",
    "        posts_csv = ( # Construye el nombre de archivos que ya se han creado en el primer chunk\n",
    "            f\"data/posts_{sub}_{mode}.csv\"\n",
    "            if mode != \"top\" else\n",
    "            f\"data/posts_{sub}_{mode}_{time_filter}.csv\"\n",
    "        )\n",
    "        with open(posts_csv, newline=\"\", encoding=\"utf-8-sig\") as f: # Lee archivos post_csv\n",
    "            r = csv.DictReader(f, delimiter=\";\")\n",
    "            for row in r:\n",
    "                pid = row[\"id\"]\n",
    "                post_index[pid] = {\n",
    "                    \"subreddit\": sub,\n",
    "                    \"title\": row.get(\"title\", \"\"),\n",
    "                    \"url\": row.get(\"url\", \"\"),\n",
    "                }\n",
    "    linked_rows = [] # Acá se guardarán las filas combinadas (comentarios + otros datos)\n",
    "    orphans = 0\n",
    "    for sub in subreddits: # Recorre cada subreddit y arma el path del csv de comentarios\n",
    "        comments_csv = f\"data/comments_{sub}.csv\"\n",
    "        if not Path(comments_csv).exists(): # Si el archivo no existe, lo salta\n",
    "            continue\n",
    "        with open(comments_csv, newline=\"\", encoding=\"utf-8-sig\") as f: # Abre el csv de comentarios\n",
    "            r = csv.DictReader(f, delimiter=\";\")\n",
    "            for c in r: # Itera cada comentario y lo relaciona con el post madre (por medio de 'post_index')\n",
    "                pid = c[\"post_id\"]\n",
    "                info = post_index.get(pid)\n",
    "                if not info:\n",
    "                    orphans += 1  # comentario cuyo post no encontramos\n",
    "                    continue\n",
    "                #Construye la fila combinada y lo agrega a 'linked_rows'\n",
    "                linked_rows.append({\n",
    "                    \"subreddit\": info[\"subreddit\"],\n",
    "                    \"post_id\": pid,\n",
    "                    \"post_title\": info[\"title\"],\n",
    "                    \"post_url\": info[\"url\"],\n",
    "                    \"comment_body\": c.get(\"body\", \"\"),\n",
    "                    \"comment_score\": c.get(\"score\", 0),\n",
    "                })\n",
    "\n",
    "    # Escribe y guarda archivo csv combinado \n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"subreddit\",\"post_id\",\"post_title\",\"post_url\",\"comment_body\",\"comment_score\"],\n",
    "            delimiter=\";\",\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        w.writeheader()\n",
    "        w.writerows(linked_rows)\n",
    "\n",
    "    print(f\"Comentarios linkeados guardados en: {out_path} (filas: {len(linked_rows)}, huérfanos omitidos: {orphans})\")\n",
    "\n",
    "# Ejecuta la función, definiendo los parámetros de la corrida\n",
    "subreddits = [\"politics\", \"PoliticalDiscussion\", \"worldnews\"]\n",
    "MODE = \"hot\"         # también podría set \"top\"\n",
    "TIME_FILTER = \"day\"  # solo si MODE == \"top\"\n",
    "build_linked_comments_csv(subreddits, mode=MODE, time_filter=TIME_FILTER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce85071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (selenium_env)",
   "language": "python",
   "name": "selenium_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
